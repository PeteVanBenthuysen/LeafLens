{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48dfef5b",
   "metadata": {},
   "source": [
    "# DAT 402 Project 2\n",
    "\n",
    "# Plant Disease Detection from Leaf Images\n",
    "\n",
    "---\n",
    "\n",
    "### Team Members:\n",
    "- Pete VanBenthuysen\n",
    "- Emily Szolnoki\n",
    "---\n",
    "\n",
    "### Project Goal:\n",
    "The goal of this project is to predict whether a plant — specifically bell pepper, potato, or tomato — is healthy or afflicted by a specific disease based on an image of its leaf.\n",
    "Rather than simply detecting the presence of disease, the model is designed to identify the exact type of illness, enabling farmers to make informed decisions about whether a plant is salvageable, treatable, or needs to be removed.\n",
    "\n",
    "This project leverages deep learning to create a rapid, low-cost diagnostic tool for agriculture.\n",
    "Early and accurate detection of plant diseases in these critical crops is essential for preventing crop loss, maintaining food supply chains, and promoting sustainable farming practices.\n",
    "\n",
    "Traditional methods of diagnosing plant diseases can be slow, require expert knowledge, and are often inaccessible to farmers in rural or resource-limited settings.\n",
    "By training a convolutional neural network (CNN) on a large dataset of real-world agricultural images, we aim to automate disease identification across a wide range of conditions with high accuracy.\n",
    "\n",
    "This project demonstrates how machine learning can transform agriculture by making expert-level diagnostics accessible to everyone — helping farmers make faster, more informed decisions, reduce unnecessary pesticide use, improve crop yields, and enhance global food security.\n",
    "It also opens pathways for building mobile applications or embedded systems that bring AI-powered diagnostics directly into the hands of agricultural workers worldwide.\n",
    "\n",
    "---\n",
    "\n",
    "### Dataset:\n",
    "We are using the \"PlantVillage\" dataset, sourced from Kaggle.  \n",
    "The dataset contains over 20,000 labeled images of plant leaves, specifically focusing on three crops: bell peppers, potatoes, and tomatoes.\n",
    "Each image is classified into one of 15 categories, representing either a specific disease or a healthy leaf.\n",
    "The dataset is organized into folders based on the class label, with each folder containing images for a particular disease or healthy condition.\n",
    "There are no missing values.\n",
    "However, the distribution of images across classes is imbalanced, with certain diseases (such as Tomato Yellow Leaf Curl Virus) having significantly more samples than others (such as Potato healthy).\n",
    "\n",
    "You can download the dataset [Here.](https://www.kaggle.com/datasets/emmarex/plantdisease)\n",
    "\n",
    "After downloading, extract the contents into the `data/` folder of the project, preserving the folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fa07c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CONFIG] DATA_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\data\n",
      "[CONFIG] MASKED_DATA_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\\masked_data\n",
      "[CONFIG] OUTPUTS_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\n",
      "[CONFIG] RF_SPLITS_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\\splits_rf\n",
      "[CONFIG] CNN_SPLITS_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\\splits_cnn_transformed\n",
      "[CONFIG] CNN_VAL_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\\splits_cnn\\val\n",
      "[CONFIG] CNN_TEST_DIR set to: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\outputs\\splits_cnn\\test\n",
      "Using config from: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\src\\config.py\n",
      "DATA_DIR path: C:\\Users\\petev\\OneDrive\\Desktop\\LeafLens\\data\n",
      "TensorFlow version: 2.19.0\n",
      "Built with CUDA: False\n",
      "Built with GPU support: False\n",
      "Available GPUs: []\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import collections\n",
    "\n",
    "# Define path to src\n",
    "src_path = Path(os.getcwd()).resolve().parent / 'src'\n",
    "\n",
    "# Add src/ to sys.path if needed\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import config\n",
    "import config\n",
    "\n",
    "# Directory check\n",
    "print(f\"Using config from: {config.__file__}\")\n",
    "print(f\"DATA_DIR path: {config.DATA_DIR}\")\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Built with GPU support:\", tf.test.is_built_with_gpu_support())\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for gpu in physical_devices:\n",
    "    print(\"GPU device name:\", gpu.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b9ad9",
   "metadata": {},
   "source": [
    "- collections reference, path reference, clean code pathing reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a27ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import random\n",
    "import json\n",
    "import shutil\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Data preprocessing and augmentation\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Machine learning models and metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Project-specific functions\n",
    "from functions import (\n",
    "    plot_crop_distribution,\n",
    "    apply_leaf_mask,\n",
    "    mask_and_save_all_images,\n",
    "    is_split_complete,\n",
    "    save_cnn_split,\n",
    "    show_random_images,\n",
    "    plot_compressed_scatter,\n",
    "    plot_cluster_scatter,\n",
    "    plot_autoencoder_loss_curve,\n",
    "    plot_bottleneck_vs_loss,\n",
    "    plot_final_autoencoder_loss,\n",
    "    build_autoencoder\n",
    ")\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38af19e",
   "metadata": {},
   "source": [
    "Imports and Setup:\n",
    "We start by importing all necessary libraries for data handling, visualization, machine learning, and deep learning.\n",
    "We also check if a GPU is available to speed up training.\n",
    "\n",
    "Setting Random Seeds:\n",
    "To ensure reproducibility across multiple runs, random seeds are set for `torch`, `numpy`, and `random` early in the project.  \n",
    "This controls random processes such as data splitting, model initialization, and sampling. This helps ensure consistent and comparable results across experiments. [1] \n",
    "\n",
    "[1] [PyTorch Reproducibility Guide](https://pytorch.org/docs/stable/notes/randomness.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfb28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the main PlantVillage folder\n",
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"data\", \"PlantVillage\"))\n",
    "\n",
    "# Define the path to the duplicate PlantVillage folder\n",
    "duplicate_path = os.path.join(base_path, \"PlantVillage\")\n",
    "\n",
    "# Check if the duplicate folder exists\n",
    "if os.path.exists(duplicate_path):\n",
    "    print(f\"Duplicate folder found at: {duplicate_path}\")\n",
    "    # Remove the duplicate folder and its contents\n",
    "    shutil.rmtree(duplicate_path)\n",
    "    print(f\"Duplicate folder removed: {duplicate_path}\")\n",
    "else:\n",
    "    print(f\"No duplicate folder found at: {duplicate_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16b194",
   "metadata": {},
   "source": [
    "This code was implemented to address the issue of duplicate data in the PlantVillage dataset. Specifically, there was a nested `PlantVillage` folder inside the main `PlantVillage` directory, which contained duplicate data. This duplication could lead to:\n",
    "\n",
    "- Data Redundancy: The same images being processed multiple times, which could skew the results of the analysis or model training.\n",
    "- Increased Storage Usage: Unnecessary duplication of files increases storage requirements.\n",
    "- Performance Issues: Processing duplicate data could slow down the pipeline and introduce inefficiencies.\n",
    "\n",
    "The code ensures that the nested duplicate folder is identified and removed, leaving only the main `PlantVillage` folder with its subfolders intact.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271eb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DATA_DIR from the config.py file\n",
    "from config import DATA_DIR\n",
    "\n",
    "# Path to the PlantVillage folder\n",
    "plant_village_path = config.DATA_DIR / \"PlantVillage\"\n",
    "\n",
    "# List categories (subfolders inside PlantVillage)\n",
    "categories = sorted([folder.name for folder in plant_village_path.iterdir() if folder.is_dir()])\n",
    "print(f\"Number of categories (classes): {len(categories)}\")\n",
    "print(\"Classes:\", categories)\n",
    "\n",
    "# Load images and labels\n",
    "images = []\n",
    "labels = []\n",
    "skipped_images = {}  # Dictionary to track skipped images per category\n",
    "\n",
    "for label_idx, category in enumerate(categories):\n",
    "    category_path = plant_village_path / category  # Path to the subfolder\n",
    "    skipped_images[category] = 0  # Initialize skipped count for this category\n",
    "    for img_name in os.listdir(category_path):  # Iterate over files in the subfolder\n",
    "        img_path = category_path / img_name  # Full path to the image\n",
    "        if img_path.is_file():  # Ensure it's a file (not a directory)\n",
    "            try:\n",
    "                img = Image.open(img_path).convert('RGB')  # Open the image\n",
    "                img = img.resize((128, 128))  # Resize to a consistent shape\n",
    "                img_array = np.array(img)  # Convert to a NumPy array\n",
    "                images.append(img_array)  # Add the image to the list\n",
    "                labels.append(label_idx)  # Add the corresponding label\n",
    "            except (IOError, OSError) as e:\n",
    "                print(f\"Skipped invalid image {img_path} under category '{category}': {e}\")\n",
    "                skipped_images[category] += 1  # Increment skipped count for this category\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Print basic dataset characteristics\n",
    "print(f\"\\nTotal images loaded: {images.shape[0]}\")\n",
    "print(f\"Each image shape: {images.shape[1:]} (Height x Width x Channels)\")\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "# Print skipped images summary\n",
    "print(\"\\n--- Skipped Images Summary ---\")\n",
    "for category, count in skipped_images.items():\n",
    "    print(f\"Category '{category}': {count} images skipped\")\n",
    "\n",
    "# Plot a bar chart of class distribution\n",
    "labels_df = pd.DataFrame({'label': labels})\n",
    "class_counts = labels_df['label'].value_counts().sort_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(categories)), class_counts.values, tick_label=categories, color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Number of Images per Class')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.grid(axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print observations\n",
    "print(\"\\n--- Observations ---\")\n",
    "print(f\"The dataset covers {len(categories)} different plant disease and healthy categories, primarily across bell peppers, potatoes, and tomatoes.\")\n",
    "print(\"Images are consistently resized to 128x128 pixels with 3 color channels (RGB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b622fc",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The PlantVillage dataset is organized into 15 distinct classes, with each class represented as a subfolder under the `PlantVillage` directory.  \n",
    "Each subfolder corresponds to a specific plant disease or a healthy plant condition across bell peppers, potatoes, and tomatoes.\n",
    "\n",
    "### Data Loading and Cleaning Steps\n",
    "\n",
    "- For each class, all files within its subfolder were iterated.\n",
    "- Only valid image files (non-directories) were processed.\n",
    "- Images were opened with the Pillow (PIL) library, converted to RGB, and resized to 128×128 pixels. [2]\n",
    "- Invalid or unreadable files (e.g., corrupted or non-image files) were safely skipped without interrupting the loading process.\n",
    "- Skipped images were tracked per category for auditing purposes.\n",
    "\n",
    "### Dataset Characteristics\n",
    "\n",
    "- Total of 20,638 valid images successfully loaded.\n",
    "- Each image is a NumPy array of shape (128, 128, 3).\n",
    "- Labels were assigned as integer IDs (0–14) based on the alphabetical order of class names.\n",
    "\n",
    "### Observations\n",
    "\n",
    "- The dataset exhibits a class imbalance, with diseases like *Tomato Yellow Leaf Curl Virus* overrepresented compared to others like *Potato Healthy*. \n",
    "- Addressing class imbalance during model training (e.g., through data augmentation or class-weighted loss) is important to prevent bias.\n",
    "- The dataset is rich, diverse, and well-structured, providing a strong foundation for developing machine learning models, particularly Convolutional Neural Networks (CNNs) for image classification tasks.\n",
    "\n",
    "[2] [Pillow documentation](https://realpython.com/image-processing-with-the-python-pillow-library/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c4ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart of class distribution\n",
    "# Group categories into healthy and infected for each crop\n",
    "crops = {\n",
    "    \"Potato\": {\"healthy\": 0, \"infected\": {}},\n",
    "    \"Pepper\": {\"healthy\": 0, \"infected\": {}},\n",
    "    \"Tomato\": {\"healthy\": 0, \"infected\": {}}\n",
    "}\n",
    "\n",
    "# Map numeric indices in class_counts to category names\n",
    "for label_idx, count in class_counts.items():\n",
    "    category = categories[label_idx]  # Get the category name from the index\n",
    "    if \"Potato\" in category:\n",
    "        if \"healthy\" in category.lower():\n",
    "            crops[\"Potato\"][\"healthy\"] += count\n",
    "        else:\n",
    "            crops[\"Potato\"][\"infected\"][category] = count\n",
    "    elif \"Pepper\" in category:\n",
    "        if \"healthy\" in category.lower():\n",
    "            crops[\"Pepper\"][\"healthy\"] += count\n",
    "        else:\n",
    "            crops[\"Pepper\"][\"infected\"][category] = count\n",
    "    elif \"Tomato\" in category:\n",
    "        if \"healthy\" in category.lower():\n",
    "            crops[\"Tomato\"][\"healthy\"] += count\n",
    "        else:\n",
    "            crops[\"Tomato\"][\"infected\"][category] = count\n",
    "\n",
    "\n",
    "# Plot bar charts for each crop\n",
    "for crop_name, crop_data in crops.items():\n",
    "    print(f\"\\n--- {crop_name} Dataset ---\")\n",
    "    print(f\"Healthy: {crop_data['healthy']} images\")\n",
    "    for category, count in crop_data[\"infected\"].items():\n",
    "        print(f\"Infected ({category}): {count} images\")\n",
    "    plot_crop_distribution(crop_name, crop_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923ef77",
   "metadata": {},
   "source": [
    "Across the datasets, a notable class imbalance between healthy and infected samples exists.\n",
    "Potato shows a severe shortage of healthy samples (152 vs 2000 infected), while Pepper is more balanced.\n",
    "Tomato is strongly dominated by infected samples, with several disease classes individually surpassing the number of healthy images.\n",
    "Such imbalances must be considered carefully during model development to avoid biased learning outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7417b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask all images\n",
    "mask_and_save_all_images()\n",
    "\n",
    "# Path to masked dataset\n",
    "masked_plant_village_path = config.MASKED_DATA_DIR / \"PlantVillage\"\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through masked images\n",
    "for label_idx, category in enumerate(sorted([folder.name for folder in masked_plant_village_path.iterdir() if folder.is_dir()])):\n",
    "    category_path = masked_plant_village_path / category\n",
    "    for img_name in os.listdir(category_path):\n",
    "        img_path = category_path / img_name\n",
    "        if img_path.is_file():\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize((128, 128))\n",
    "            img_array = np.array(img)\n",
    "            images.append(img_array)\n",
    "            labels.append(label_idx)\n",
    "\n",
    "# Convert lists to arrays\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(f\"Loaded {len(images)} masked images.\")\n",
    "print(f\"Unique classes found: {len(np.unique(labels))}\")\n",
    "\n",
    "config.MASKED_ARRAYS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "np.save(config.MASKED_ARRAYS_DIR / 'images.npy', images)\n",
    "np.save(config.MASKED_ARRAYS_DIR / 'labels.npy', labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cef368",
   "metadata": {},
   "source": [
    "Talk about here how we are masking the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b167f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of images and labels\n",
    "print(f\"Number of images: {images.shape[0]}\")\n",
    "print(f\"Number of labels: {len(labels)}\")\n",
    "\n",
    "# Ensure the number of labels matches the number of images\n",
    "assert len(labels) == images.shape[0], \"Mismatch between the number of images and labels!\"\n",
    "\n",
    "# Flatten and normalize pixel values\n",
    "X_flat = images.reshape(images.shape[0], -1) / 255.0\n",
    "print(f\"Shape after flattening: {X_flat.shape}\")\n",
    "\n",
    "# Split for training the Autoencoder\n",
    "train_images, val_images = train_test_split(X_flat, test_size=0.2, random_state=42)\n",
    "\n",
    "input_dim = train_images.shape[1]\n",
    "\n",
    "# List of bottleneck sizes to test\n",
    "bottleneck_candidates = [100, 200, 300, 400, 500]\n",
    "bottleneck_results = {}\n",
    "\n",
    "# Train Autoencoders with different bottlenecks\n",
    "for bottleneck_dim in bottleneck_candidates:\n",
    "    print(f\"Training Autoencoder with bottleneck_dim = {bottleneck_dim}\")\n",
    "\n",
    "    autoencoder = build_autoencoder(input_dim, bottleneck_dim)\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    history = autoencoder.fit(\n",
    "        train_images, train_images,\n",
    "        epochs=100,\n",
    "        batch_size=256,\n",
    "        validation_data=(val_images, val_images),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Get final validation loss\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    bottleneck_results[bottleneck_dim] = final_val_loss\n",
    "\n",
    "# Plot bottleneck size vs final validation loss\n",
    "\n",
    "plot_bottleneck_vs_loss(\n",
    "    bottleneck_results,\n",
    "    save_path=config.EDA_DIR / 'bottleneck_vs_loss.png'\n",
    ")\n",
    "\n",
    "# Choose best bottleneck\n",
    "\n",
    "best_bottleneck_dim = min(bottleneck_results, key=bottleneck_results.get)\n",
    "print(f\"Best bottleneck dimension selected: {best_bottleneck_dim}\")\n",
    "\n",
    "# Save bottleneck optimization results to JSON\n",
    "\n",
    "optimization_results = {\n",
    "    \"all_bottlenecks\": bottleneck_results,\n",
    "    \"best_bottleneck_dim\": best_bottleneck_dim,\n",
    "    \"best_val_loss\": bottleneck_results[best_bottleneck_dim]\n",
    "}\n",
    "\n",
    "with open(config.EDA_DIR / 'bottleneck_optimization_results.json', 'w') as f:\n",
    "    json.dump(optimization_results, f, indent=4)\n",
    "\n",
    "print(f\"Saved optimization results JSON to {config.EDA_DIR / 'bottleneck_optimization_results.json'}\")\n",
    "\n",
    "# Retrain final autoencoder with best bottleneck\n",
    "\n",
    "autoencoder = build_autoencoder(input_dim, best_bottleneck_dim)\n",
    "\n",
    "history = autoencoder.fit(\n",
    "    train_images, train_images,\n",
    "    epochs=100,\n",
    "    batch_size=256,\n",
    "    validation_data=(val_images, val_images),\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save final optimized loss curve\n",
    "\n",
    "plot_final_autoencoder_loss(\n",
    "    history,\n",
    "    best_bottleneck_dim,\n",
    "    save_path=config.EDA_DIR / 'optimized_autoencoder_loss_curve.png'\n",
    ")\n",
    "\n",
    "print(f\"Saved final optimized loss curve to {config.EDA_DIR / 'optimized_autoencoder_loss_curve.png'}\")\n",
    "\n",
    "# Compress all images using final trained model\n",
    "\n",
    "bottleneck_model = models.Model(inputs=autoencoder.input, outputs=autoencoder.get_layer('bottleneck').output)\n",
    "compressed_features = bottleneck_model.predict(X_flat, batch_size=256)\n",
    "\n",
    "print(f\"Compressed features shape: {compressed_features.shape}\")\n",
    "\n",
    "# Apply t-SNE for 2D visualization\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "features_2d = tsne.fit_transform(compressed_features)\n",
    "\n",
    "print(f\"Shape after t-SNE reduction: {features_2d.shape}\")\n",
    "\n",
    "# Encode true labels numerically\n",
    "unique_labels = np.unique(labels)\n",
    "label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "numeric_labels = np.array([label_to_int[label] for label in labels])\n",
    "\n",
    "# Plot compressed feature space colored by true labels\n",
    "plot_compressed_scatter(\n",
    "    features_2d,\n",
    "    numeric_labels,\n",
    "    title='True Labels on Autoencoder Compressed Features',\n",
    "    save_path=config.EDA_DIR / 'true_labels_autoencoder_scatter.png'\n",
    ")\n",
    "\n",
    "# Perform KMeans clustering\n",
    "kmeans = KMeans(n_clusters=15, random_state=42)\n",
    "clusters = kmeans.fit_predict(compressed_features)\n",
    "print(f\"Cluster assignments shape: {clusters.shape}\")\n",
    "\n",
    "# Plot compressed feature space colored by KMeans clusters\n",
    "plot_compressed_scatter(\n",
    "    features_2d,\n",
    "    clusters,\n",
    "    title='KMeans Clustering on Autoencoder Compressed Features',\n",
    "    save_path=config.EDA_DIR / 'kmeans_autoencoder_scatter.png'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6340b870",
   "metadata": {},
   "source": [
    "mention we are going with a hard cap of 300, not the n value because it is computationally too expensive to run but were showing graphs with that 90% variance n value (depends what the n value is)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e604d0fe",
   "metadata": {},
   "source": [
    "### PCA Visualization Analysis\n",
    "\n",
    "After flattening the leaf images and applying Principal Component Analysis (PCA) to reduce the dimensionality, we visualized the resulting principal components in a 2D scatter plot.  \n",
    "Each point in the plot represents an individual leaf image, positioned based on its major sources of pixel variance captured by PCA.\n",
    "\n",
    "#### Dataset and Processing Overview:\n",
    "- Number of images: 20,638\n",
    "- Number of labels: 20,638\n",
    "- Flattened feature size: 49,152 features per image (128×128×3)\n",
    "- PCA reduced shape: (20,638, 2) (two principal components per image)\n",
    "- Cluster assignments shape (after K-Means): (20,638,)\n",
    "\n",
    "The scatter plot reveals that while some classes form partially distinct clusters, there is significant overlap among most categories.  \n",
    "This overlap indicates that raw pixel intensity patterns alone are not sufficient for clean separation of plant diseases, highlighting the limitations of pixel-level features.  \n",
    "Such findings reinforce the importance of applying deeper feature extraction techniques, such as convolutional layers in CNNs, to learn more abstract and discriminative representations from images.\n",
    "\n",
    "Interestingly, one distinct group of images appears isolated from the main distribution, suggesting that certain diseases—likely those with very unique visual symptoms—are more easily distinguishable even in low-dimensional pixel space.  \n",
    "Overall, the PCA visualization provides valuable initial insights into the structure and complexity of the dataset, setting the stage for further analysis through clustering and supervised deep learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eba3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "images_normalized = images / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "# Combine images and labels into a single dataset for stratified splitting\n",
    "data = list(zip(images_normalized, labels))\n",
    "\n",
    "# Split the dataset into train, validation, and test sets with stratification\n",
    "train_data, temp_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=0.5, random_state=42, stratify=[label for _, label in temp_data]\n",
    ")\n",
    "\n",
    "# Debugging: Check split sizes\n",
    "print(f\"Train size: {len(train_data)}, Temp size: {len(temp_data)}\")\n",
    "print(f\"Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n",
    "\n",
    "# Unzip the data back into images and labels\n",
    "X_train, y_train = zip(*train_data)\n",
    "X_val, y_val = zip(*val_data)\n",
    "X_test, y_test = zip(*test_data)\n",
    "\n",
    "# Convert back to NumPy arrays\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "# Debugging: Check label distributions\n",
    "print(f\"Train labels distribution: {collections.Counter(y_train)}\")\n",
    "print(f\"Validation labels distribution: {collections.Counter(y_val)}\")\n",
    "print(f\"Test labels distribution: {collections.Counter(y_test)}\")\n",
    "\n",
    "# Save CNN splits\n",
    "cnn_split_dir = Path(config.OUTPUTS_DIR) / \"splits_cnn\"\n",
    "\n",
    "\n",
    "if not cnn_split_dir.exists() or not all(\n",
    "    is_split_complete(cnn_split_dir / split_name) for split_name in [\"train\", \"val\", \"test\"]\n",
    "):  # Check if all splits are complete\n",
    "    print(\"CNN splits are incomplete or missing. Regenerating...\")\n",
    "    shutil.rmtree(cnn_split_dir, ignore_errors=True)  # Clear the directory if it exists\n",
    "    cnn_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    print(f\"Saving training data to: {cnn_split_dir / 'train'}\")\n",
    "    save_cnn_split(cnn_split_dir, X_train, y_train, \"train\")\n",
    "\n",
    "    print(f\"Saving validation data to: {cnn_split_dir / 'val'}\")\n",
    "    save_cnn_split(cnn_split_dir, X_val, y_val, \"val\")\n",
    "\n",
    "    print(f\"Saving test data to: {cnn_split_dir / 'test'}\")\n",
    "    save_cnn_split(cnn_split_dir, X_test, y_test, \"test\")\n",
    "\n",
    "    print(\"CNN splits saved in outputs folder.\")\n",
    "else:\n",
    "    print(\"CNN splits already exist and are complete. Skipping save.\")\n",
    "\n",
    "# Save RF Splits\n",
    "rf_split_dir = Path(config.OUTPUTS_DIR) / \"splits_rf\"\n",
    "if not rf_split_dir.exists() or not any(rf_split_dir.iterdir()):  # Check if directory exists or is empty\n",
    "    rf_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Flatten the images\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "    X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    # Scale the flattened images\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_flat)  # Fit scaler on training data\n",
    "    X_val_scaled = scaler.transform(X_val_flat)          # Transform validation data\n",
    "    X_test_scaled = scaler.transform(X_test_flat)        # Transform test data\n",
    "\n",
    "    # Fit PCA on the scaled training data\n",
    "    pca = PCA(n_components=300)  # Capped at 300 components for RF\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)  # Fit and transform training data\n",
    "\n",
    "    # Transform validation and test data using the PCA fitted on training data\n",
    "    X_val_pca = pca.transform(X_val_scaled)  # Transform validation data\n",
    "    X_test_pca = pca.transform(X_test_scaled)  # Transform test data\n",
    "\n",
    "    # Save RF splits\n",
    "    np.save(rf_split_dir / \"X_train_pca.npy\", X_train_pca)\n",
    "    np.save(rf_split_dir / \"X_val_pca.npy\", X_val_pca)\n",
    "    np.save(rf_split_dir / \"X_test_pca.npy\", X_test_pca)\n",
    "    np.save(rf_split_dir / \"y_train.npy\", y_train)\n",
    "    np.save(rf_split_dir / \"y_val.npy\", y_val)\n",
    "    np.save(rf_split_dir / \"y_test.npy\", y_test)\n",
    "\n",
    "    print(\"RF splits saved in outputs folder (with scaling before PCA).\")\n",
    "else:\n",
    "    print(\"RF splits already exist and are not empty. Skipping save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac68cc",
   "metadata": {},
   "source": [
    "# Dataset Preparation for CNN and RF Models\n",
    "\n",
    "This code prepares the dataset for both a Convolutional Neural Network (CNN) and a Random Forest (RF) model by performing data preprocessing, organizing splits, and saving the data in the appropriate formats. By moving the data into the outputs folder, we preserve the processed data used during our EDA and ensure consistent inputs for modeling.\n",
    "\n",
    "## 1. Normalizing Image Data\n",
    "The images are normalized by dividing the pixel values by 255.  \n",
    "This scales pixel values to the range \\([0, 1]\\), which improves model training efficiency and convergence, especially for neural networks like CNNs.\n",
    "\n",
    "## 2. Splitting the Dataset\n",
    "The dataset is split into training, validation, and test sets:\n",
    "\n",
    "- First, 80% of the data is assigned to the training set.\n",
    "- The remaining 20% is evenly split into validation and test sets (10% each).\n",
    "\n",
    "To maintain consistent class distributions across all splits, stratified sampling is applied:\n",
    "\n",
    "- For Random Forest (RF) data, stratification is based on the labels.\n",
    "- For CNN data, the same stratified splits are used to ensure each class is proportionally represented across train, validation, and test sets.\n",
    "\n",
    "This helps prevent class imbalance during training and evaluation. [3]\n",
    "\n",
    "## 3. Preparing and Saving CNN Data\n",
    "Before saving the data for the CNN model, the script checks if the CNN splits already exist in the output directory to avoid overwriting.  \n",
    "If not, it:\n",
    "\n",
    "- Saves normalized images as `.png` files.\n",
    "- Organizes images into train, validation, and test folders.\n",
    "- Further separates each folder into subfolders by class label (e.g., `class_0`, `class_1`, etc.).\n",
    "\n",
    "Images are rescaled back to \\([0, 255]\\) when saving to maintain proper image formats.\n",
    "\n",
    "**Note:**  \n",
    "PCA is not applied to CNN data.  \n",
    "The CNN model works directly with the normalized pixel values and full image structure.\n",
    "\n",
    "## 4. Preparing and Saving RF Data with PCA\n",
    "For the Random Forest model:\n",
    "\n",
    "- The images are first flattened from 3D arrays \\((128, 128, 3)\\) into 1D vectors.\n",
    "- The datasets are then scaled so all features contribute equally.\n",
    "- PCA is then applied to reduce the dimensionality of these flattened vectors.\n",
    "\n",
    "PCA is fit only on the training data to avoid data leakage:\n",
    "\n",
    "- The PCA model is trained (fit) using only `X_train`.\n",
    "- The same fitted PCA is then used to transform `X_val` and `X_test`.\n",
    "\n",
    "After PCA transformation:\n",
    "\n",
    "- The reduced-dimension arrays (`X_train_pca`, `X_val_pca`, `X_test_pca`) are saved as `.npy` files.\n",
    "- Corresponding labels (`y_train`, `y_val`, `y_test`) are also saved.\n",
    "\n",
    "## 5. Overview\n",
    "This code carefully splits and saves the dataset, preparing two clean and separate pipelines:\n",
    "\n",
    "- The CNN model will train directly on normalized images organized by class folders.\n",
    "- The Random Forest model will train on PCA-reduced feature vectors, preventing data leakage and improving interpretability.\n",
    "\n",
    "All steps ensure consistency, class balance, and proper separation between training, validation, and testing phases.\n",
    "\n",
    "\n",
    "[3] [Stratifying both features and labels](https://carpentries-incubator.github.io/intro-image-classification-cnn/instructor/aio.html?utm_source=chatgpt.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations for training data\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=30),  # Randomly rotate images by ±30 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Adjust brightness, contrast, etc.\n",
    "    transforms.RandomResizedCrop(size=(128, 128), scale=(0.8, 1.0)),  # Randomly crop and resize\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally\n",
    "    transforms.ToTensor()  # Convert image to tensor\n",
    "])\n",
    "\n",
    "# Function to check if a folder contains the expected data\n",
    "def is_folder_complete(folder_path, labels):\n",
    "    folder_path = Path(folder_path)\n",
    "    if not folder_path.exists():\n",
    "        return False  # Folder doesn't exist, so it's not complete\n",
    "\n",
    "    # Check if all class folders exist and contain files\n",
    "    for label in set(labels):  # Unique class labels\n",
    "        class_dir = folder_path / f\"class_{label}\"\n",
    "        if not class_dir.exists() or not any(class_dir.iterdir()):  # Check if folder exists and is not empty\n",
    "            return False  # Missing or empty class folder\n",
    "\n",
    "    return True  # Folder is complete\n",
    "\n",
    "# Function to apply transformations and save augmented images\n",
    "def apply_transformations_and_save(data, labels, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for idx, (img_array, label) in enumerate(zip(data, labels)):\n",
    "        img = Image.fromarray((img_array * 255).astype(np.uint8))  # Convert back to PIL image\n",
    "        transformed_img = transform(img)  # Apply transformations\n",
    "        transformed_img = transforms.ToPILImage()(transformed_img)  # Convert back to PIL for saving\n",
    "        \n",
    "        # Save the transformed image\n",
    "        class_dir = output_dir / f\"class_{label}\"\n",
    "        class_dir.mkdir(parents=True, exist_ok=True)\n",
    "        transformed_img.save(class_dir / f\"img_{idx}_transformed.png\")  # Add '_transformed' to the filename\n",
    "\n",
    "# Transform and Save Training Data\n",
    "cnn_train_dir_transformed = Path(config.OUTPUTS_DIR) / \"splits_cnn_transformed\" / \"train\"\n",
    "\n",
    "if not is_folder_complete(cnn_train_dir_transformed, y_train):\n",
    "    print(\"Transformed CNN training data is incomplete or missing. Regenerating...\")\n",
    "    shutil.rmtree(cnn_train_dir_transformed, ignore_errors=True)  # Clear only the transformed training directory\n",
    "    apply_transformations_and_save(X_train, y_train, cnn_train_dir_transformed)  # Augment training set\n",
    "    print(\"Transformed CNN training data saved.\")\n",
    "else:\n",
    "    print(\"Transformed CNN training data already exists and is complete. Skipping transformation.\")\n",
    "\n",
    "# Debugging: Check validation and test directories\n",
    "cnn_val_dir = Path(config.OUTPUTS_DIR) / \"splits_cnn\" / \"val\"\n",
    "cnn_test_dir = Path(config.OUTPUTS_DIR) / \"splits_cnn\" / \"test\"\n",
    "\n",
    "if not cnn_val_dir.exists() or not any(cnn_val_dir.iterdir()):\n",
    "    print(f\"Validation directory is missing or empty: {cnn_val_dir}\")\n",
    "else:\n",
    "    print(f\"Validation directory exists and contains files: {cnn_val_dir}\")\n",
    "\n",
    "if not cnn_test_dir.exists() or not any(cnn_test_dir.iterdir()):\n",
    "    print(f\"Test directory is missing or empty: {cnn_test_dir}\")\n",
    "else:\n",
    "    print(f\"Test directory exists and contains files: {cnn_test_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f624a81",
   "metadata": {},
   "source": [
    "This code applies data augmentation specifically to the CNN dataset splits by creating new, transformed versions of the training images. It is intended to improve model generalization by simulating real-world variability in the leaf images, such as changes in rotation, lighting, cropping, and orientation.\n",
    "\n",
    "First, a transformation pipeline is defined using `transforms.Compose`. The pipeline randomly rotates images by ±30 degrees, applies random color jitter to adjust brightness, contrast, saturation, and hue, randomly resizes and crops images to 128x128 pixels, randomly flips images horizontally with a probability of 50%, and finally converts the augmented image to a tensor format. [4] [5]\n",
    "\n",
    "The code then defines a function to check if a folder contains the expected data. If the folder is incomplete or missing, it is cleared and regenerated to ensure that the augmented images are up-to-date and complete.\n",
    "\n",
    "A second function is defined to apply the transformations to each image and save the newly transformed versions into a new output directory. Each image is augmented individually, and the augmented images are saved under their respective class folders, with filenames labeled to indicate that they are transformed versions.\n",
    "\n",
    "Next, the script checks whether the output folders for the transformed training dataset already exists. If it does not exist, the code clears or creates the appropriate folders and applies the transformations to the training dataset. If the folder already exists, it skips the transformation process to avoid redundant computation.\n",
    "\n",
    "The training dataset for the CNN will now come from the splits_cnn_transformed directory, where the images have been augmented. However, the validation and test set images will continue to come from the original splits_cnn directory without any augmentation. This ensures that the model is evaluated on completely unaltered, real-world test images, maintaining a fair evaluation standard.\n",
    "\n",
    "Meanwhile, the Random Forest (RF) dataset remains completely unchanged. No transformations or augmentations are applied to the RF data, as Random Forests are trained on flattened, non-image arrays where spatial information and visual transformations are not relevant.\n",
    "\n",
    "In summary, this code ensures that the CNN model benefits from additional variability and robustness during training, while the RF model and the CNN test evaluation remain based on the original unaltered data. This careful separation of augmented and original data helps maintain training integrity while improving generalization.\n",
    "\n",
    "[4] [Fine Tuning Transforms](https://rumn.medium.com/ultimate-guide-to-fine-tuning-in-pytorch-part-3-deep-dive-to-pytorch-data-transforms-53ed29d18dde)\n",
    "[5] [Transforming and Augmenting Images](https://rumn.medium.com/ultimate-guide-to-fine-tuning-in-pytorch-part-3-deep-dive-to-pytorch-data-transforms-53ed29d18dde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24692d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels\n",
    "y_train = np.load(config.RF_SPLITS_DIR / 'y_train.npy')\n",
    "y_val = np.load(config.RF_SPLITS_DIR / 'y_val.npy')\n",
    "y_test = np.load(config.RF_SPLITS_DIR / 'y_test.npy')\n",
    "\n",
    "# Count classes\n",
    "print(\"Random Forest - Train Class Counts:\", collections.Counter(y_train))\n",
    "print(\"Random Forest - Val Class Counts:\", collections.Counter(y_val))\n",
    "print(\"Random Forest - Test Class Counts:\", collections.Counter(y_test))\n",
    "\n",
    "\n",
    "# CNN splits: Check image imbalance\n",
    "\n",
    "def count_images_per_class(directory):\n",
    "    directory = Path(directory)\n",
    "    if not directory.exists():\n",
    "        print(f\"Directory does not exist: {directory}\")\n",
    "        return {}  # Return an empty dictionary if the directory is missing\n",
    "\n",
    "    counts = {}\n",
    "    for class_folder in os.listdir(directory):\n",
    "        class_path = os.path.join(directory, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            counts[class_folder] = len(os.listdir(class_path))\n",
    "    return counts\n",
    "\n",
    "# CNN Transformed splits (train)\n",
    "cnn_train_counts = count_images_per_class(config.CNN_SPLITS_DIR / 'train')\n",
    "\n",
    "# CNN Validation/Test split (untouched)\n",
    "cnn_val_counts = count_images_per_class(config.CNN_VAL_DIR)\n",
    "cnn_test_counts = count_images_per_class(config.CNN_TEST_DIR)\n",
    "\n",
    "# Print counts\n",
    "print(\"\\nCNN - Train Class Counts:\", cnn_train_counts)\n",
    "print(\"CNN - Val Class Counts:\", cnn_val_counts)\n",
    "print(\"CNN - Test Class Counts:\", cnn_test_counts)\n",
    "\n",
    "# Plot class distribution for RF splits\n",
    "def plot_rf_class_distribution(labels, title):\n",
    "    \n",
    "    class_counts = Counter(labels)  # Count occurrences of each class\n",
    "    classes = list(class_counts.keys())\n",
    "    nums = list(class_counts.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, nums, color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting RF class distributions\n",
    "plot_rf_class_distribution(y_train, 'RF Train Set Class Distribution')\n",
    "plot_rf_class_distribution(y_val, 'RF Validation Set Class Distribution')\n",
    "plot_rf_class_distribution(y_test, 'RF Untouched Test Set Class Distribution')\n",
    "\n",
    "# Plot class distribution for CNN splits\n",
    "\n",
    "def plot_class_distribution(counts, title, x_labels=None):\n",
    "   \n",
    "    if x_labels is None:\n",
    "        x_labels = list(counts.keys())\n",
    "    nums = [counts.get(label, 0) for label in x_labels]  # Use 0 for missing labels\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(x_labels, nums, color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plotting\n",
    "plot_class_distribution(cnn_train_counts, 'CNN Transformed Train Set Class Distribution')\n",
    "plot_class_distribution(cnn_val_counts, 'CNN Validation Set Class Distribution')\n",
    "plot_class_distribution(cnn_test_counts, 'CNN Untouched Test Set Class Distribution')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b5069",
   "metadata": {},
   "source": [
    "We first imported the paths to our Random Forest and CNN datasets using the config file to keep everything organized and flexible. We then loaded the Random Forest label arrays and CNN datasets and counted the number of samples for each class across the training, validation, and untouched test sets to confirm that stratification worked properly. We then plotted the class distributions to visually inspect whether any classes are severely underrepresented, helping us determine if additional data augmentation or balancing techniques might be needed before training our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e38db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "original_X_train_path = config.RF_SPLITS_DIR / 'X_train_pca.npy'\n",
    "original_y_train_path = config.RF_SPLITS_DIR / 'y_train.npy'\n",
    "resampled_X_train_path = config.RF_SPLITS_DIR / 'X_train_pca_resampled.npy'\n",
    "resampled_y_train_path = config.RF_SPLITS_DIR / 'y_train_resampled.npy'\n",
    "metadata_path = config.RF_SPLITS_DIR / 'resampling_metadata.json'\n",
    "\n",
    "# Helper function to check if a file exists and is not empty\n",
    "def is_valid_file(path):\n",
    "    return path.exists() and path.stat().st_size > 0\n",
    "\n",
    "# Check if resampling needs to be applied\n",
    "if not (is_valid_file(resampled_X_train_path) and is_valid_file(resampled_y_train_path)):\n",
    "    # Load the RF dataset splits\n",
    "    X_train = np.load(original_X_train_path)\n",
    "    y_train = np.load(original_y_train_path)\n",
    "\n",
    "    # Print class distribution before SMOTETomek\n",
    "    print(\"Class distribution before SMOTETomek:\")\n",
    "    print(Counter(y_train))\n",
    "\n",
    "    # Apply SMOTETomek to balance the dataset\n",
    "    smote_tomek = SMOTETomek(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Print class distribution after SMOTETomek\n",
    "    print(\"\\nClass distribution after SMOTETomek:\")\n",
    "    print(Counter(y_train_resampled))\n",
    "\n",
    "    # Save the resampled data (no scaling, as it's already scaled)\n",
    "    np.save(resampled_X_train_path, X_train_resampled)\n",
    "    np.save(resampled_y_train_path, y_train_resampled)\n",
    "\n",
    "    # Save metadata with more detailed info (e.g., shape of resampled data)\n",
    "    metadata = {\n",
    "        \"original_class_distribution\": dict(Counter(map(int, y_train))),\n",
    "        \"resampled_class_distribution\": dict(Counter(map(int, y_train_resampled))),\n",
    "        \"resampling_method\": \"SMOTETomek\",\n",
    "        \"random_state\": 42,\n",
    "        \"original_data_shape\": X_train.shape,\n",
    "        \"resampled_data_shape\": X_train_resampled.shape  # Added this for better clarity\n",
    "    }\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    print(\"\\nResampled training data has been saved, along with metadata.\")\n",
    "\n",
    "else:\n",
    "    print(\"Resampled data already exists and is valid. Skipping SMOTETomek.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b525679d",
   "metadata": {},
   "source": [
    "In this code, we are preparing the training set for our Random Forest model by first checking if resampled data already exists. We define the file paths for the original training features and labels, as well as the intended paths for the resampled versions and metadata. To ensure reliability, we use a helper function called is_valid_file that checks whether a file not only exists but also is not empty, preventing errors caused by incomplete or corrupted files.\n",
    "\n",
    "If the resampled training data does not already exist or is invalid, we proceed by loading the original PCA-transformed training features (`X_train_pca.npy`) and labels (`y_train.npy`). We print the class distribution before resampling so we can observe the existing imbalance across classes, which often hurts model performance.\n",
    "\n",
    "To correct the class imbalance, we apply SMOTETomek, a hybrid resampling technique. SMOTETomek works in two stages: first, SMOTE (Synthetic Minority Oversampling Technique) generates synthetic samples for minority classes to make them better represented; second, Tomek Links identifies and removes overlapping or borderline samples between classes, cleaning noisy examples from the dataset. As a result, SMOTETomek not only increases the number of samples for underrepresented classes but also improves the quality of class boundaries by removing ambiguous samples. [6]\n",
    "\n",
    "Importantly, because Tomek links remove samples even after SMOTE has added synthetic data, the final class counts are similar but not exactly equal across classes. Some classes that originally had borderline examples may end up with slightly fewer samples than others. This is expected behavior and is considered beneficial because it reduces noise and prevents the model from overfitting to bad examples. [7]\n",
    "\n",
    "After resampling, we save the resampled, scaled features and corresponding labels separately to disk, ensuring they are preserved for later use. In addition, we generate and save a metadata.json file that records the original and resampled class distributions, the resampling method used, and the random state, providing clear documentation of how the training data was prepared.\n",
    "\n",
    "If the resampled data already exists and is valid, the code simply skips reprocessing, printing a message to indicate this.\n",
    "\n",
    "[6] [SmoteTOMEK](https://imbalanced-learn.org/stable/references/generated/imblearn.combine.SMOTETomek.html)\n",
    "[7] [Rescaling After SMOTE](https://imbalanced-learn.org/stable/over_sampling.html#preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936f7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your directory\n",
    "rf_split_dir = Path(config.OUTPUTS_DIR) / \"splits_rf\"\n",
    "\n",
    "# List of file names you want to check\n",
    "file_names = [\n",
    "    \"X_train_pca.npy\",\n",
    "    \"X_train_pca_resampled.npy\",\n",
    "    \"X_val_pca.npy\",\n",
    "    \"X_test_pca.npy\",\n",
    "    \"y_train.npy\",\n",
    "    \"y_train_resampled.npy\",\n",
    "    \"y_val.npy\",\n",
    "    \"y_test.npy\"\n",
    "]\n",
    "\n",
    "# Loop through each file and print the dtype\n",
    "print(\"Confirmation of data types and shapes:\\n\")\n",
    "\n",
    "for file_name in file_names:\n",
    "    file_path = rf_split_dir / file_name\n",
    "    data = np.load(file_path)\n",
    "    print(f\"{file_name}: dtype = {data.dtype}, shape = {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5272e4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
